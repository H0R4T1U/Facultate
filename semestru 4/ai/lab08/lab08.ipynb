{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e6a0d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /Users/horatiubanciu/Library/Python/3.9/lib/python/site-packages (25.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12bc19e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'Ok lar... Joking wif u oni...']\n",
      "['spam', 'ham']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "crtDir = os.getcwd()\n",
    "fileName = os.path.join(crtDir, 'data', 'spam.csv')\n",
    "\n",
    "data = []\n",
    "with open(fileName, encoding='latin-1') as csv_file:  # Specify the correct encoding\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            dataNames = row\n",
    "        else:\n",
    "            data.append(row)\n",
    "        line_count += 1\n",
    "\n",
    "\n",
    "inputs = [data[i][0] for i in range(len(data))][:100]\n",
    "outputs = [data[i][1] for i in range(len(data))][:100]\n",
    "labelNames = list(set(outputs))\n",
    "\n",
    "print(inputs[:2])\n",
    "print(labelNames[:2])\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da8ae77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(5)\n",
    "noSamples = len(inputs)\n",
    "indexes = [i for i in range(noSamples)]\n",
    "trainSample = np.random.choice(indexes, int(0.8 * noSamples), replace= False)\n",
    "testSample = [i for i in indexes if not i in trainSample]\n",
    "\n",
    "trainInputs = [inputs[i] for i in trainSample]\n",
    "trainOutputs = [outputs[i] for i in trainSample]\n",
    "testInputs = [inputs[i] for i in testSample]\n",
    "testOutputs = [outputs[i] for i in testSample]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a8c576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TF-IDF Feature Extract\n",
      "Vocabular size with TF-IDF: 50 words\n",
      "Feature matrix shape: (80, 50)\n",
      "Some vocabulary words: ['all' 'already' 'and' 'anything' 'at' 'be' 'but' 'call' 'can' 'did']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print(\"\\n TF-IDF Feature Extract\")\n",
    "vectorizer = TfidfVectorizer(max_features=50)\n",
    "trainFeatures = vectorizer.fit_transform(trainInputs)\n",
    "testFeatures = vectorizer.transform(testInputs)\n",
    "print(f\"Vocabular size with TF-IDF: {len(vectorizer.vocabulary_)} words\")\n",
    "print(f\"Feature matrix shape: {trainFeatures.shape}\")\n",
    "print(f\"Some vocabulary words: {vectorizer.get_feature_names_out()[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f501d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word To Vec Feature Extraction\n",
      "Word2Vec features shape: (80, 100)\n",
      "Sample vector: [ 0.00120308  0.00019636 -0.00106943 -0.00109585  0.00257067]...\n"
     ]
    }
   ],
   "source": [
    "print(\"Word To Vec Feature Extraction\")\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "def preprocess_text(texts):\n",
    "    tokenized_texts = []\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokenized_texts.append(tokens)\n",
    "    return tokenized_texts\n",
    "\n",
    "tokenized_train = preprocess_text(trainInputs)\n",
    "w2v_model = Word2Vec(sentences=tokenized_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def document_vector(doc, model):\n",
    "    words = [word for word in doc if word in model.wv.key_to_index]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model.wv[word] for word in words], axis=0)\n",
    "\n",
    "\n",
    "train_vectors = [document_vector(doc, w2v_model) for doc in tokenized_train]\n",
    "tokenized_test = preprocess_text(testInputs)\n",
    "test_vectors = [document_vector(doc, w2v_model) for doc in tokenized_test]\n",
    "\n",
    "train_vectors_w2v = np.array(train_vectors)\n",
    "test_vectors_w2v = np.array(test_vectors)\n",
    "\n",
    "print(f\"Word2Vec features shape: {train_vectors_w2v.shape}\")\n",
    "print(f\"Sample vector: {train_vectors_w2v[0][:5]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cbc34f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Text Features\n",
      "Additional features shape: (80, 13)\n",
      "Sample additional features: [1.24000000e+02 2.30000000e+01 5.39130435e+00 2.41935484e-02\n",
      " 0.00000000e+00 8.06451613e-02 0.00000000e+00 1.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.68000000e-01 8.32000000e-01\n",
      " 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(\"Extra Text Features\")\n",
    "\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "use_nltk_sentiment = True\n",
    "\n",
    "\n",
    "def extract_text_features(texts):\n",
    "    features = []\n",
    "    for text in texts:\n",
    "        length = len(text)\n",
    "        word_count = len(text.split())\n",
    "        avg_word_length = length / max(1, word_count)\n",
    "\n",
    "        uppercase_ratio = sum(1 for c in text if c.isupper()) / max(1, len(text))\n",
    "        digit_ratio = sum(1 for c in text if c.isdigit()) / max(1, len(text))\n",
    "        special_chars = sum(1 for c in text if not c.isalnum() and not c.isspace()) / max(1, len(text))\n",
    "\n",
    "        exclamation_count = text.count('!')\n",
    "        question_count = text.count('?')\n",
    "\n",
    "        has_url = 1 if 'http' in text.lower() or 'www' in text.lower() else 0\n",
    "        has_urgent = 1 if 'urgent' in text.lower() or 'now' in text.lower() else 0\n",
    "\n",
    "        feature_list = [length, word_count, avg_word_length, uppercase_ratio, \n",
    "                        digit_ratio, special_chars, exclamation_count, question_count,\n",
    "                        has_url, has_urgent]\n",
    "        \n",
    "        if use_nltk_sentiment:\n",
    "            sentiment = sia.polarity_scores(text)\n",
    "            feature_list.extend([sentiment['pos'], sentiment['neu'], sentiment['neg']])\n",
    "        features.append(feature_list)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "train_additional_features = extract_text_features(trainInputs)\n",
    "test_additional_features = extract_text_features(testInputs)\n",
    "\n",
    "print(f\"Additional features shape: {train_additional_features.shape}\")\n",
    "print(f\"Sample additional features: {train_additional_features[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f846cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCIKIT CLASSIFICATION\n",
      "Training MLP classifier: (80, 163)\n",
      "Accuracy with scikit-learn MLP: 0.9500\n",
      "\n",
      " Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        spam       0.94      1.00      0.97        17\n",
      "         ham       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.97      0.83      0.89        20\n",
      "weighted avg       0.95      0.95      0.95        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SCIKIT CLASSIFICATION\")\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(trainOutputs)\n",
    "test_labels = label_encoder.fit_transform(testOutputs)\n",
    "\n",
    "train_features_combined = np.hstack((trainFeatures.toarray(), train_vectors_w2v, train_additional_features))\n",
    "test_features_combined = np.hstack((testFeatures.toarray(), test_vectors_w2v, test_additional_features))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features_combined)\n",
    "test_features_scaled = scaler.fit_transform(test_features_combined)\n",
    "\n",
    "print(f\"Training MLP classifier: {train_features_scaled.shape}\")\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='adaptive', max_iter=500, random_state=42)\n",
    "mlp.fit(train_features_scaled, train_labels)\n",
    "\n",
    "predictions = mlp.predict(test_features_scaled)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "\n",
    "print(f\"Accuracy with scikit-learn MLP: {accuracy:.4f}\")\n",
    "print(\"\\n Report:\")\n",
    "print(classification_report(test_labels, predictions, target_names=labelNames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d147c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training custom MLP classifier with input shape: (80, 163)\n",
      "Iteration 0, Loss: 8.111504\n",
      "\n",
      "Accuracy with custom ANN: 0.6500\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        spam       0.86      0.71      0.77        17\n",
      "         ham       0.17      0.33      0.22         3\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.51      0.52      0.50        20\n",
      "weighted avg       0.75      0.65      0.69        20\n",
      "\n",
      "\n",
      "Sample Predictions:\n",
      "Text: As per your request 'Melle Melle (Oru Minnaminungi...\n",
      "True label: ham, Predicted label: spam\n",
      "--------------------------------------------------\n",
      "Text: WINNER!! As a valued network customer you have bee...\n",
      "True label: spam, Predicted label: spam\n",
      "--------------------------------------------------\n",
      "Text: XXXMobileMovieClub: To use your credit, click the ...\n",
      "True label: spam, Predicted label: spam\n",
      "--------------------------------------------------\n",
      "Text: Oh k...i'm watching here:)...\n",
      "True label: ham, Predicted label: ham\n",
      "--------------------------------------------------\n",
      "Text: Did you catch the bus ? Are you frying an egg ? Di...\n",
      "True label: ham, Predicted label: spam\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from MyANN import MyMLPClassifier\n",
    "train_features_combined = np.hstack((trainFeatures.toarray(), train_vectors_w2v, train_additional_features))\n",
    "test_features_combined = np.hstack((testFeatures.toarray(), test_vectors_w2v, test_additional_features))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features_combined)\n",
    "test_features_scaled = scaler.transform(test_features_combined)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(trainOutputs)\n",
    "test_labels = label_encoder.transform(testOutputs)\n",
    "\n",
    "print(f\"\\nTraining custom MLP classifier with input shape: {train_features_scaled.shape}\")\n",
    "my_mlp = MyMLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  \n",
    "    activation='relu',             \n",
    "    learning_rate_init=0.001,     \n",
    "    max_iter=100,                  \n",
    "    random_state=42,              \n",
    "    verbose=True                   \n",
    ")\n",
    "\n",
    "my_mlp.fit(train_features_scaled, train_labels)\n",
    "\n",
    "predictions = my_mlp.predict(test_features_scaled).flatten()\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"\\nAccuracy with custom ANN: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, predictions, target_names=labelNames))\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i in range(min(5, len(testInputs))):\n",
    "    text = testInputs[i]\n",
    "    true_label = testOutputs[i]\n",
    "    predicted_label = labelNames[predictions[i]]\n",
    "    print(f\"Text: {text[:50]}...\")\n",
    "    print(f\"True label: {true_label}, Predicted label: {predicted_label}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0a5af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
